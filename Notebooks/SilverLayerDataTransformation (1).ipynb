{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29573259-f5ad-4599-9919-ad90adfac784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bd4787-6b62-41ad-bcc6-8e545447bee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Physician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c231952-5c7b-44a8-95ab-1a7b8c71ca12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import re\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''')\n",
    "spark.sql('''USE SCHEMA silver''')\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"Physician_LookUp\",\n",
    "    comment=\"Transformed dimphysician table for US Healthcare analytics, with clear column names, type handling, and audit columns included\"\n",
    ")\n",
    "@dlt.expect(\"dim_physician_pk_not_null\", \"dim_physician_pk IS NOT NULL\")\n",
    "def dimphysician():\n",
    "    # Read from bronze layer\n",
    "    df = dlt.read(\"ushealthcaredynamics.bronze.dimphysician\")\n",
    "\n",
    "    # Rename columns to snake_case\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in df.columns}\n",
    "    df = df.select([col(c).alias(renamed_cols[c]) for c in df.columns])\n",
    "\n",
    "    # Apply transformations\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .withColumn(\"dim_physician_pk\", col(\"dim_physician_pk\").cast(\"int\"))\n",
    "        .withColumn(\"provider_npi\", col(\"provider_npi\").cast(\"int\"))\n",
    "        .withColumn(\"speciality_code\", col(\"speciality_code\").cast(\"int\"))\n",
    "        .withColumn(\"provider_fte\", round(col(\"provider_fte\").cast(\"float\"), 2))\n",
    "        .withColumn(\"provider_name\", col(\"provider_name\").cast(\"string\"))\n",
    "        .filter(col(\"dim_physician_pk\").isNotNull())\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fcb210b-0242-4e9a-be0f-1e905f96eb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Payer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7bd993f-47a6-4c8b-a27a-d8d9b614907a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"Payer_LookUp\",\n",
    "    comment=\"Transformed dimpayor table for US Healthcare analytics, with clear column names, type handling, and audit columns included\"\n",
    ")\n",
    "def dimpayor():\n",
    "    df = dlt.read(\"ushealthcaredynamics.bronze.dimpayor\")\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in df.columns}\n",
    "    df = df.select([col(c).alias(renamed_cols[c]) for c in df.columns])\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .withColumn(\"dim_payer_pk\", col(\"dim_payer_pk\").cast(\"int\"))\n",
    "        .withColumn(\"payer_name\", col(\"payer_name\").cast(\"string\"))\n",
    "        .filter(col(\"dim_payer_pk\").isNotNull())\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b586c287-55f0-4c1d-9a7f-25e1d5d4326c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759e3e7e-c79f-40a4-b062-b8eab239b945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "@dlt.table(\n",
    "    name=\" Transaction_LookUp\",\n",
    "    comment=\"Transformed dimtransaction table for US Healthcare analytics, with clear column names, type handling, and audit columns included\"\n",
    ")\n",
    "def dimtransaction():\n",
    "    df = dlt.read(\"ushealthcaredynamics.bronze.dimtransaction\")\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in df.columns}\n",
    "    df = df.select([col(c).alias(renamed_cols[c]) for c in df.columns])\n",
    "    df_transformed = (\n",
    "        df\n",
    "        .withColumn(\"dim_transaction_pk\", col(\"dim_transaction_pk\").cast(\"int\"))\n",
    "        .withColumn(\"transaction_type\", col(\"transaction_type\").cast(\"string\"))\n",
    "        .withColumn(\"transaction\", col(\"transaction\").cast(\"string\"))\n",
    "        .withColumn(\"adjustment_reason\", col(\"adjustment_reason\").cast(\"string\"))\n",
    "        .filter(col(\"dim_transaction_pk\").isNotNull())\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc6c7abe-d6f0-4880-bab5-b818c25b0780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83624df-e7c9-4694-8eb9-e2b940dc8fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, to_date, current_timestamp\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"DiagnosisCode_LookUp\",\n",
    "    comment=\"Cleaned diagnosis data for Capstone\",\n",
    "    table_properties={\"quality\": \"DimDiagnosis silver\"}\n",
    ")\n",
    "def DimDiagnosis_silver():\n",
    "    df = dlt.read(\"ushealthcaredynamics.bronze.dimdiagnosis\")\n",
    "    df = df.toDF(*[c.replace(' ', '_') for c in df.columns])\n",
    "    df = df.filter(col(\"dimDiagnosisCodePK\").isNotNull())\n",
    "    df = df.dropDuplicates([\"dimDiagnosisCodePK\"])\n",
    "    df = df.withColumn(\"dimDiagnosisCodePK\", col(\"dimDiagnosisCodePK\").cast(\"long\"))\n",
    "    dlt.expect(\"PK_not_null\", \"dimDiagnosisCodePK IS NOT NULL\")\n",
    "    df = df.withColumn(\"update_date\", current_timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac4b9716-5f4a-445f-9206-fcd8d3708ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6456813a-170d-41f4-a9cb-460cb0f9bfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lpad\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dimDate\",\n",
    "    comment=\"Cleaned and standardized date dimension data.\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def DimDate_silver():\n",
    "  \"\"\"\n",
    "  Reads the bronze DimDate table, filters out null dates, renames/standardizes columns,\n",
    "  and casts the 'Date' column to the DateType.\n",
    "  \"\"\"\n",
    "  return (\n",
    "    dlt.read(\"ushealthcaredynamics.bronze.dimdate\")\n",
    "    # 1. Filter out records where 'Date' is null before processing\n",
    "    .filter(col(\"Date\").isNotNull())\n",
    "    .select(      \n",
    "      # Rename remaining columns\n",
    "      col(\"Year\").alias(\"date_year\"),\n",
    "      col(\"Month\").alias(\"date_month_name\"),\n",
    "      \n",
    "      # Use lpad to ensure MonthPeriod is a two-digit string (e.g., '1' -> '01')\n",
    "      lpad(col(\"MonthPeriod\"), 2, '0').alias(\"date_month_number\"), \n",
    "      \n",
    "      col(\"MonthYear\").alias(\"date_month_year\"),\n",
    "      col(\"Day\").alias(\"date_day_of_month\"),\n",
    "      col(\"Date\").alias(\"Date\"),\n",
    "      col(\"DayName\").alias(\"date_day_name\"),\n",
    "      \n",
    "      # Keep audit columns\n",
    "      col(\"ingestion_date\"),\n",
    "      col(\"source_file\"),\n",
    "      col(\"audit_source\")\n",
    "    )\n",
    "  )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d357b4-3219-43fc-b859-66b9fb7225f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a3327c-1379-4d84-a82c-1a64b486a010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, when, expr, upper, substring, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "# Define a masking UDF for PatientNumber\n",
    "def mask_mrn(mrn):\n",
    "    if mrn is None:\n",
    "        return None\n",
    "    mrn_str = str(mrn)\n",
    "    if len(mrn_str) <= 4:\n",
    "        return '*' * len(mrn_str)\n",
    "    return '*' * (len(mrn_str) - 4) + mrn_str[-4:]\n",
    "\n",
    "mask_mrn_udf = udf(mask_mrn, StringType())\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "@dlt.table(name=\"Patient_LookUp\")\n",
    "def silver_dimpatient():\n",
    "    bronze_df = dlt.read(\"ushealthcaredynamics.bronze.dimpatient\")\n",
    "    # Rename columns to snake_case\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in bronze_df.columns}\n",
    "    df = bronze_df.select([col(c).alias(renamed_cols[c]) for c in bronze_df.columns])\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"patient_email_domain\", expr(\"substring(email, length(email) - 9, 10)\"))\n",
    "          .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "          .withColumn(\"patient_state_code\", upper(substring(col(\"state_code\"),1,3)))\n",
    "          .withColumn(\"patient_age_group\",\n",
    "                      when(col(\"patient_age\") < 18, \"Child\")\n",
    "                      .when((col(\"patient_age\") >= 18) & (col(\"patient_age\") < 65), \"Adult\")\n",
    "                      .otherwise(\"Senior\"))\n",
    "          .withColumn(\"patmrn\", mask_mrn_udf(col(\"patient_number\")))\n",
    "          .dropDuplicates([\"dim_patient_pk\"])\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d9c577-0857-41a0-bb08-ff2a9d1fe2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CPTCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c69d07-7f3e-439d-aacf-e95c4b0fdd94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, upper, trim, initcap, substring, when, current_timestamp\n",
    "import re\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "# Utility to convert camelCase/PascalCase to snake_case\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "@dlt.table(name=\"CPTCode_LookUp\")\n",
    "def silver_dimcptcode():\n",
    "    bronze_df = dlt.read(\"ushealthcaredynamics.bronze.dimcptcode\")\n",
    "    # Rename all columns to snake_case\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in bronze_df.columns}\n",
    "    df = bronze_df.select([col(c).alias(renamed_cols[c]) for c in bronze_df.columns])\n",
    "\n",
    "    df = (\n",
    "        df.dropDuplicates([\"cpt_code\"])\n",
    "          .withColumn(\"cpt_code\", upper(trim(col(\"cpt_code\"))))\n",
    "          .withColumn(\"cpt_desc\", initcap(trim(col(\"cpt_desc\"))))\n",
    "          .withColumn(\"short_desc\", substring(col(\"cpt_desc\"), 1, 30))\n",
    "          .withColumn(\"is_cpt_code_missing\", col(\"cpt_code\").isNull() | (col(\"cpt_code\") == \"\"))\n",
    "          .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10302b92-2c43-4190-9f5e-8a97e150bb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3fca74-76de-46be-9ccc-47964e967972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, initcap, trim, current_timestamp\n",
    "import re\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "# Utility for snake_case conversion\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "@dlt.table(name=\"Hospital_LookUp\")\n",
    "def silver_dimhospital():\n",
    "    bronze_df = dlt.read(\"ushealthcaredynamics.bronze.dimhospital\")\n",
    "    # Rename both columns to snake_case\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in bronze_df.columns}\n",
    "    df = bronze_df.select([col(c).alias(renamed_cols[c]) for c in bronze_df.columns])\n",
    "    df = (\n",
    "        df.withColumn(\"location_name\", initcap(trim(col(\"location_name\"))))\n",
    "          .dropDuplicates([\"dim_location_pk\"])\n",
    "          .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9d5a68f-dc6b-495f-8c2a-946a0d7fa236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FactTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09266241-b01b-476b-8315-e14390c1d15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import re\n",
    "from pyspark.sql.functions import col, to_date, round, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark.sql('''USE CATALOG ushealthcaredynamics''');\n",
    "spark.sql('''USE SCHEMA silver''');\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    s = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    s = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s)\n",
    "\n",
    "    # Remove multiple underscores\n",
    "    s = re.sub('_+', '_', s)\n",
    "\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "def to_snake_case_df(df):\n",
    "    renamed_cols = {c: camel_to_snake(c) for c in df.columns}\n",
    "    return df.select([col(c).alias(renamed_cols[c]) for c in df.columns])\n",
    "\n",
    "\n",
    "\n",
    "def mask_mrn(mrn):\n",
    "    if mrn is None:\n",
    "        return None\n",
    "    mrn_str = str(mrn)\n",
    "    if len(mrn_str) <= 4:\n",
    "        return '*' * len(mrn_str)\n",
    "    return '*' * (len(mrn_str) - 4) + mrn_str[-4:]\n",
    "\n",
    "mask_mrn_udf = udf(mask_mrn, StringType())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Single Silver Table\n",
    "# ------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    comment=\"Single-step silver table with snake_case, filtering, casting, and MRN masking.\"\n",
    ")\n",
    "def facttable():\n",
    "\n",
    "    # Read bronze\n",
    "    df = dlt.read(\"ushealthcaredynamics.bronze.facttable\")\n",
    "\n",
    "    # Filter out missing key fields\n",
    "    df_filtered = df.filter(\n",
    "        col(\"FactTablePK\").isNotNull() & (col(\"FactTablePK\") != \"\") &\n",
    "        col(\"dimPatientPK\").isNotNull() & (col(\"dimPatientPK\") != \"\") &\n",
    "        col(\"dimPhysicianPK\").isNotNull() & (col(\"dimPhysicianPK\") != \"\") &\n",
    "        col(\"dimDateServicePK\").isNotNull() & (col(\"dimDateServicePK\") != \"\") &\n",
    "        col(\"dimDatePostPK\").isNotNull() & (col(\"dimDatePostPK\") != \"\") &\n",
    "        col(\"dimCPTCodePK\").isNotNull() & (col(\"dimCPTCodePK\") != \"\") &\n",
    "        col(\"dimPayerPK\").isNotNull() & (col(\"dimPayerPK\") != \"\") &\n",
    "        col(\"dimTransactionPK\").isNotNull() & (col(\"dimTransactionPK\") != \"\") &\n",
    "        col(\"dimLocationPK\").isNotNull() & (col(\"dimLocationPK\") != \"\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    df_snake = to_snake_case_df(df_filtered)\n",
    "\n",
    "   \n",
    "    df_final = (\n",
    "        df_snake\n",
    "        .withColumn(\"fact_table_pk\", col(\"fact_table_pk\").cast(\"long\"))\n",
    "        .withColumn(\"check_dimension\", col(\"check_dimension\").cast(\"long\"))\n",
    "        .withColumn(\"dim_patient_pk\", col(\"dim_patient_pk\").cast(\"long\"))\n",
    "        .withColumn(\"dim_physician_pk\", col(\"dim_physician_pk\").cast(\"long\"))\n",
    "        .withColumn(\"dim_date_service_pk\", to_date(col(\"dim_date_service_pk\"), \"dd-MM-yyyy\"))\n",
    "        .withColumn(\"dim_date_post_pk\", to_date(col(\"dim_date_post_pk\"), \"dd-MM-yyyy\"))\n",
    "        .withColumn(\"dim_cpt_code_pk\", col(\"dim_cpt_code_pk\").cast(\"long\"))\n",
    "        .withColumn(\"dim_payer_pk\", col(\"dim_payer_pk\").cast(\"long\"))\n",
    "        .withColumn(\"dim_transaction_pk\", col(\"dim_transaction_pk\").cast(\"long\"))\n",
    "        .withColumn(\"dim_location_pk\", col(\"dim_location_pk\").cast(\"long\"))\n",
    "\n",
    "        \n",
    "        .withColumn(\"patient_number\", mask_mrn_udf(col(\"patient_number\")))\n",
    "\n",
    "        .withColumn(\"dim_diagnosis_code_pk\", col(\"dim_diagnosis_code_pk\").cast(\"long\"))\n",
    "        .withColumn(\"cpt_units\", col(\"cpt_units\").cast(\"int\"))\n",
    "        .withColumn(\"gross_expenses\", col(\"gross_expenses\").cast(\"double\"))\n",
    "        .withColumn(\"adjustment\", col(\"adjustment\").cast(\"double\"))\n",
    "        .withColumn(\"insurance_payment\", col(\"insurance_payment\").cast(\"int\"))\n",
    "        .withColumn(\"patient_payment\", col(\"patient_payment\").cast(\"double\"))\n",
    "        .withColumn(\"ar\", round(col(\"ar\").cast(\"double\"), 2))\n",
    "        .withColumn(\"ingestion_date\", col(\"ingestion_date\").cast(\"timestamp\"))\n",
    "        .withColumn(\"source_file\", col(\"source_file\").cast(\"string\"))\n",
    "        .withColumn(\"audit_source\", col(\"audit_source\").cast(\"string\"))\n",
    "    )\n",
    "\n",
    "    return df_final\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SilverLayerDataTransformation (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
